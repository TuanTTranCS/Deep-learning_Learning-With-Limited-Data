{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testbed_challenge1_ResNet9_gridsearch_FINAL",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6DxGkPeQDD"
      },
      "source": [
        "***COMP 691 - Deep Learning project - Challenge 1***\n",
        "\n",
        "*Team: DL_DT_Explore*\n",
        "\n",
        "*Members*:\n",
        "* Trong-Tuan Tran\n",
        "* Hussein Abdallah\n",
        "* Manh-Quoc-Dat Le\n",
        "\n",
        "This Jupyter Notebook implements the approach: ResNet9 model with Cossine Loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hfgxjTd_lk"
      },
      "source": [
        "Here the goal is to train on 100 samples. In this preliminary testbed the evaluation will be done on a 2000 sample validation set. Note in the end the final evaluation will be done on the full CIFAR-10 test set as well as potentially a separate dataset. The validation samples here should not be used for training in any way, the final evaluation will provide only random samples of 100 from a datasource that is not the CIFAR-10 training data. \n",
        "\n",
        "Initial configurations & hyperparameters used for grid search (params defined in lists [ ] will be used for grid search):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53EaHq45ztBo"
      },
      "source": [
        "import time\n",
        "from numpy.random import RandomState\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Epochs: 300 - lr: - 0.001 - dropout: 0 - Weight_decay: 1e-05 - Grad_clip: 0.005 \n",
        "# Scenario 17/300 - Epochs: 700 - lr: - 0.0001 - dropout: 0 - Weight_decay: 0.00016051911333587627 - Grad_clip: 0.015119336467640998\n",
        "# Scenario 19/300 - Epochs: 700 - lr: - 0.0001 - dropout: 0 - Weight_decay: 0.00016051911333587627 - Grad_clip: 0.02576638574613588\n",
        "\n",
        "epochs_list = [700]\n",
        "grad_clips = [0.02576638574613588] # Gradient clipping\n",
        "weight_decays = [0.00016051911333587627] # Weight decay for Adam Optimizer\n",
        "lrs = [0.0001] # Learning rates\n",
        "drop_outs = [0] # Value for drop out layers\n",
        "batch_size = 128\n",
        "runs = 5 # Number of instances to run the train and test to evaluate mean and std dev of test accuracy\n",
        "epoch_display_range = 100\n",
        "search_plot = True # Define whether to run evaluation after each epochs to plot accuracy trend or not\n",
        "log_enabled = False # Define if log down progress to support resume from previous completed run or not (for grid search)\n",
        "save_image = True # If search_plot is True, enable this will save the plotted image to the img_path define below  \n",
        "google_drive_mount = True # If running in Google Colab and enable this, progress and image files will save to Google Drive instead of local file\n",
        "final_eval = True # Whether to run using train set or test set for final evalutation & submission\n",
        "\n",
        "eval_str = 'DEV PHASE - ' if not final_eval else 'FINAL EVAL - '\n",
        "comment = eval_str + 'ResNet9 + Cosine Loss + modified train transforms + random search best config' # comment string to put in acc trend images\n",
        "if final_eval:\n",
        "    search_plot = False\n",
        "    save_image = False\n",
        "    google_drive_mount = False\n",
        "    log_enabled = False\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "#device = torch.device('cpu')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNn9F7oZsY35"
      },
      "source": [
        "The section below detects which environment is running (Colab, Kaggle or local computer). The output folders will be determine accordingly. If running in final_eval mode, no output file will be generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt4RKss-MKWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c997c929-bf27-4519-c72a-765e7934bf0f"
      },
      "source": [
        "import os\n",
        "output_path = 'output_txt/'\n",
        "img_path = 'img/'\n",
        "Colab = False\n",
        "Kaggle = 'kaggle' in os.getcwd()\n",
        "root = '.' # Root to download dataset\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    Colab = True  \n",
        "    from google.colab import drive\n",
        "    if not os.path.exists('/content/drive/MyDrive/') and google_drive_mount:\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "    else:\n",
        "        if google_drive_mount:\n",
        "            print('Drive already mounted at at /content/drive')\n",
        "\n",
        "    Google_path = '/content/drive/MyDrive/Colab Notebooks/COMP691_project/' if google_drive_mount else '/'\n",
        "    if not os.path.exists(Google_path):\n",
        "        os.mkdir(Google_path)\n",
        "    img_path = Google_path + img_path\n",
        "    if not os.path.exists(img_path):\n",
        "        os.mkdir(img_path)\n",
        "    output_path = Google_path + output_path  \n",
        "else:\n",
        "    if Kaggle:\n",
        "        root = '../input/cifar10'\n",
        "        output_path = ''\n",
        "        img_path = ''\n",
        "        print('Running in Kaggle')\n",
        "    else:\n",
        "        print('Not running on CoLab or Kaggle')\n",
        "output_file_name = 'report_ADAM_cosine_improve_FINAL.txt'\n",
        "output_file_path = output_path + output_file_name\n",
        "progress_file = output_path + 'grid_search_progress_FINAL.txt'\n",
        "img_file_name_prefix = output_file_name.replace('.txt', '')\n",
        "img_file_path = img_path + img_file_name_prefix + '/'\n",
        "\n",
        "if not final_eval:\n",
        "    if not os.path.exists(img_path):\n",
        "        os.mkdir(img_path)\n",
        "\n",
        "    if not os.path.exists(img_file_path):\n",
        "        os.mkdir(img_file_path)\n",
        "\n",
        "    if not os.path.exists(output_path):\n",
        "        os.mkdir(output_path)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyBTUe3idZI"
      },
      "source": [
        "Setup training/testing and other helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7soYNWEedl9"
      },
      "source": [
        "import gc\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, grad_clip=None, sched=None, display=True):\n",
        "    model.train()\n",
        "    loss_function = nn.CosineEmbeddingLoss()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data).to(device)\n",
        "\n",
        "        GT=torch.zeros((len(target),10))\n",
        "        for idx in range(len(target)):\n",
        "            GT[idx][target[idx]]=1\n",
        "\n",
        "        GT=GT.to(device)\n",
        "        \n",
        "        loss = loss_function(output, GT, torch.Tensor(output.size(0)).to(device).fill_(1.0))\n",
        "        #loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        if grad_clip:\n",
        "            nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        if sched:\n",
        "            sched.step()\n",
        "        if display and (batch_idx == 0 or batch_idx + 1 == len(train_loader)):\n",
        "          print('   Train Epoch: {} [step {}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch + 1, batch_idx + 1, len(train_loader),\n",
        "              100. * batch_idx / len(train_loader), loss.detach().item()))\n",
        "        if device == torch.device('cuda'):\n",
        "            del loss, output\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "def test(model, device, test_loader, display=True):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    loss_function = nn.CosineEmbeddingLoss()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            GT=torch.zeros((len(target),10))\n",
        "            for idx in range(len(target)):\n",
        "                GT[idx][target[idx]]=1\n",
        "\n",
        "            GT=GT.to(device)\n",
        "            \n",
        "            test_loss += loss_function(output, GT, torch.Tensor(output.size(0)).to(device).fill_(1.0)).item() # sum up batch loss\n",
        "            #test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    if display:\n",
        "        print('   Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "def plot_accs(accs_accross_runs, display_str, comment='N/A', save_img=True):\n",
        "    plt.figure();\n",
        "    #epochs = list(range(1, len(accs_accross_runs[0]) + 1))\n",
        "    max_acc_display = ''\n",
        "    for i, run_accs in enumerate(accs_accross_runs):\n",
        "        max_acc = max(run_accs)\n",
        "        max_epochs = [index + 1 for index, acc in enumerate(run_accs) if acc == max_acc]\n",
        "        if len(max_epochs) > 3:\n",
        "            not_display_count = len(max_epochs) - 3\n",
        "            max_epochs = str(max_epochs[:3]) + f'... + {not_display_count} more'\n",
        "            \n",
        "        max_acc_display = f' - max acc: {max_acc}% at epochs {max_epochs}'\n",
        "        plt.plot(run_accs, label=f'Run #{i + 1}' + max_acc_display)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('Test accuracy (%)')\n",
        "    plt.legend();\n",
        "    plt.title(f'Test accuracies for \\n{display_str}Note: {comment}');\n",
        "\n",
        "    if save_img:\n",
        "        scenario = display_str[display_str.index(' ') + 1: display_str.index('/')]\n",
        "        img_name = img_file_path + f'{scenario}' + generate_image_suffix()\n",
        "        plt.savefig(img_name, bbox_inches='tight')\n",
        "\n",
        "def generate_image_suffix():\n",
        "    return f'_{time.time()%10000000:.0f}' + '.png'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxT5P8An3Zg8"
      },
      "source": [
        "Definition of ResNet9 model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4hpe7QbQFnr"
      },
      "source": [
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "num_classes = 10\n",
        "in_channels = 3\n",
        "\n",
        "def conv_block(in_channels, out_channels, drop_out=0, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ReLU(inplace=True), nn.Dropout(drop_out)\n",
        "              ]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class NET(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes, drop_out):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = conv_block(in_channels, 64, drop_out)\n",
        "        self.conv2 = conv_block(64, 128, drop_out, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128, drop_out), conv_block(128, 128, drop_out))\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        self.conv3 = conv_block(128, 256, drop_out, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, drop_out, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512, drop_out), conv_block(512, 512, drop_out))\n",
        "        self.conv5 = conv_block(512, 1028, drop_out, pool=True)\n",
        "        self.res3 = nn.Sequential(conv_block(1028, 1028, drop_out), conv_block(1028, 1028, drop_out))\n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(2), \n",
        "                                        nn.Flatten(), \n",
        "                                        nn.Linear(1028, num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.conv5(out)\n",
        "        out = self.res3(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPjWBE4MerTX"
      },
      "source": [
        "The below tries a numbers of random problem instances defined in `runs` variable at the beginning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v7xU1HMelJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce0b68d-575d-4fa5-e423-329ea3bbdf5b"
      },
      "source": [
        "%%time\n",
        "\n",
        "device_name = torch.cuda.get_device_name(0) if device == torch.device('cuda') else 'cpu'\n",
        "\n",
        "scenario_count = len(epochs_list) * len(weight_decays) * len(lrs) * len(drop_outs) * len(grad_clips)\n",
        "\n",
        "# Statistic for CIFAR-10 datasets\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "# Data augmentation during training:\n",
        "transform_train = transforms.Compose([\n",
        "                                    transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "                                    transforms.RandomGrayscale(),\n",
        "                                    transforms.RandomHorizontalFlip(),\n",
        "                                    torchvision.transforms.RandomAffine(degrees=30),\n",
        "                                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2), \n",
        "                                    transforms.ToTensor(), \n",
        "                                    normalize]) #careful to keep this one same\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "\n",
        "print('Running on {}'.format(device_name))\n",
        "\n",
        "##### Cifar Data\n",
        "run_on_train_set = not final_eval\n",
        "dataset = 'train' if run_on_train_set else 'test'\n",
        "print(f'Using CIFAR-10 {dataset} set')\n",
        "print(comment)\n",
        "cifar_data = datasets.CIFAR10(root='.', train=run_on_train_set, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.', train=run_on_train_set, transform=transform_val, download=True)\n",
        "\n",
        "\n",
        "\n",
        "training_done = False\n",
        "count = 1\n",
        "scenario = 1\n",
        "next_run = 1\n",
        "previous_runs_accs = []\n",
        "previous_train_times = []\n",
        "previous_eval_times = []\n",
        "previous_exec_times = []\n",
        "ran_in_middle = False\n",
        "\n",
        "if log_enabled:\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as file_read:\n",
        "            progress_content = file_read.readlines()\n",
        "        # print(progress_content)\n",
        "        if progress_content[0].replace('\\n', '') == output_file_name:\n",
        "            previous_scenario = progress_content[1].replace('\\n', '')\n",
        "            previous_runs_accs = eval(progress_content[2].replace('\\n', ''))\n",
        "            previous_runs = len(previous_runs_accs)\n",
        "            print(f'Previous progress on {output_file_name} stopped at scenario {previous_scenario}/{scenario_count}' +\\\n",
        "                 f', run {previous_runs}/{runs}')\n",
        "            if previous_runs == runs: # Already complete the previous scenario\n",
        "                scenario = int(previous_scenario) + 1\n",
        "\n",
        "            else: # Previous scenario just completed partially, resume in the next run\n",
        "                ran_in_middle = True\n",
        "                scenario = int(previous_scenario)\n",
        "                next_run = previous_runs + 1\n",
        "                previous_execution_times = eval(progress_content[3].replace('\\n', ''))\n",
        "\n",
        "                for i, previous_execution_time in enumerate(previous_execution_times):\n",
        "                    previous_train_times.append(previous_execution_time[0]) \n",
        "                    previous_eval_times.append(previous_execution_time[1]) \n",
        "                    previous_exec_times.append(previous_execution_time[2])  \n",
        "            if scenario > scenario_count:\n",
        "                training_done = True\n",
        "                print('Training was already done!')\n",
        "            else:\n",
        "                print(f'Will resume training at scenario: {scenario}, run# {next_run}')\n",
        "\n",
        "if not training_done:\n",
        "    for epochs in epochs_list:\n",
        "        for lr in lrs:\n",
        "            for drop_out in drop_outs:\n",
        "                for weight_decay in weight_decays:\n",
        "                    for grad_clip in grad_clips:\n",
        "\n",
        "                        if not ran_in_middle: \n",
        "\n",
        "                            accs = []\n",
        "                            train_times = []\n",
        "                            evaluation_times = []\n",
        "                            total_times = []\n",
        "                            run_execution_times = []\n",
        "                        else:\n",
        "                            if count < scenario:\n",
        "                                count += 1\n",
        "                                continue #skip until reaching the scenario to run\n",
        "                            accs = previous_runs_accs\n",
        "                            train_times = previous_train_times\n",
        "                            evaluation_times = previous_eval_times\n",
        "                            total_times = previous_exec_times\n",
        "                            run_execution_times = previous_execution_times\n",
        "                        #scenario += 1\n",
        "\n",
        "                        print('\\nScenario %d/%d - Epochs: %d - lr: - %s - dropout: %s - Weight_decay: %s - Grad clip: %s'%(\n",
        "                            scenario, scenario_count, epochs, lr, drop_out, weight_decay, grad_clip\n",
        "                        ))\n",
        "                        accs_accross_runs_plot = []\n",
        "                        for seed in range(next_run, runs + 1):\n",
        "                            start_time = time.time()\n",
        "                            # Extract a subset of 100 (class balanced) samples per class for training and 2000 samples for validation\n",
        "                            permute_range = 5000 if run_on_train_set else 1000\n",
        "                            prng = RandomState(seed)\n",
        "                            random_permute = prng.permutation(np.arange(0, permute_range))\n",
        "                            indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:10]] for classe in range(0, 10)])\n",
        "                            indx_val = np.concatenate([np.where(np.array(cifar_data_val.targets) == classe)[0][random_permute[10:210]] for classe in range(0, 10)])\n",
        "\n",
        "\n",
        "                            train_data = Subset(cifar_data, indx_train)\n",
        "                            val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "                            print('  Run# [%d/%d] - Num Samples For Training %d - Num Samples For Val %d'%(seed, runs, train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "\n",
        "                            train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                                                        batch_size=batch_size, \n",
        "                                                                        shuffle=True)\n",
        "\n",
        "                            val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                                                    batch_size=batch_size, \n",
        "                                                                    shuffle=False)\n",
        "\n",
        "                            model = NET(in_channels, num_classes, drop_out)\n",
        "                            model.to(device)\n",
        "                            optimizer = torch.optim.Adam(model.parameters(), \n",
        "                                                        lr=lr, \n",
        "                                                        #momentum=0.9,\n",
        "                                                        weight_decay=weight_decay)\n",
        "                            sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr, epochs=epochs, \n",
        "                                                                            steps_per_epoch=len(train_loader))\n",
        "                            test_accs = []\n",
        "                            eval_time = 0\n",
        "                            for epoch in range(epochs):\n",
        "                                print_condition = epoch%epoch_display_range==0 or epoch==epochs-1\n",
        "                                train(model, device, train_loader, optimizer, epoch, grad_clip=grad_clip,\n",
        "                                    sched=sched, display=print_condition)\n",
        "                                if search_plot:\n",
        "                                    eval_start = time.time()\n",
        "                                    test_acc = test(model, device, val_loader, display=print_condition)\n",
        "                                    eval_time = time.time() - eval_start\n",
        "                                    test_accs.append(test_acc)\n",
        "\n",
        "                                    \n",
        "                            train_time = time.time() - start_time    \n",
        "                            train_times.append(train_time)\n",
        "                            final_eval_start = time.time()\n",
        "                            final_acc = test_accs[-1] if search_plot else test(model, device, val_loader)\n",
        "                            accs.append(final_acc)\n",
        "                            final_eval_time = eval_time if search_plot else time.time() - final_eval_start\n",
        "                            evaluation_times.append(final_eval_time)\n",
        "                            if search_plot:\n",
        "                                accs_accross_runs_plot.append(test_accs)\n",
        "                            total_time = time.time() - start_time\n",
        "                            total_times.append(total_time)\n",
        "                            run_execution_times.append((train_time, final_eval_time, total_time))\n",
        "                            if log_enabled:\n",
        "                                progress_str = f'{output_file_name}\\n{scenario}\\n{accs}\\n{run_execution_times}'\n",
        "                                with open(progress_file, 'w') as progress_write:\n",
        "                                    progress_write.write(progress_str)\n",
        "                            if device == torch.device('cuda'):\n",
        "                                del optimizer\n",
        "                                gc.collect()\n",
        "                                torch.cuda.empty_cache()\n",
        "                            print('  Run execution time: train: %.3f (s) - eval: %.3f (s)- total: %.3f (s)'%\\\n",
        "                                  (train_time, final_eval_time, total_time))\n",
        "                        accs = np.array(accs)\n",
        "                        train_times = np.array(train_times)\n",
        "                        evaluation_times = np.array(evaluation_times)\n",
        "                        total_times = np.array(total_times)\n",
        "                        scenario_description = 'Scenario %d/%d - Epochs: %d - lr: - %s - dropout: %s - Weight_decay: %s - Grad_clip: %s'%\\\n",
        "                        (scenario, scenario_count, epochs, lr, drop_out, weight_decay, grad_clip)\n",
        "                        accuracy_description = '\\n  Final acc over %d instances: %.2f +- %.2f%%\\n'%(runs, accs.mean(), accs.std())\n",
        "                        # print(train_times.mean(), evaluation_times.mean(), total_times.mean())\n",
        "                        display_str = '  %s'%(scenario_description) +\\\n",
        "                        '\\n  Avg execution time: train: %.3f +- %.3f (s) - eval: %.3f +- %.3f (s) - total: %.3f +- %.3f (s) on %s'%\\\n",
        "                        (train_times.mean(), train_times.std(), evaluation_times.mean(), evaluation_times.std(),\n",
        "                             total_times.mean(), total_times.std(), device_name) + accuracy_description\n",
        "                        \n",
        "                        #progress_str = f'{output_file_name}\\n{scenario}\\n{accs}'\n",
        "                        print(display_str)\n",
        "                        plot_str = display_str # scenario_description + accuracy_description\n",
        "                        if search_plot:\n",
        "                            plot_accs(accs_accross_runs_plot, plot_str, comment, save_image)\n",
        "                        if log_enabled:\n",
        "                            mode = 'a' if os.path.exists(output_file_path) else 'w'\n",
        "\n",
        "                            with open(output_file_path, mode) as output_write:\n",
        "                                output_write.write(display_str)\n",
        "                        ran_in_middle = False\n",
        "                        next_run = 1\n",
        "                        scenario += 1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on Tesla P100-PCIE-16GB\n",
            "Using CIFAR-10 test set\n",
            "FINAL EVAL - ResNet9 + Cosine Loss + modified train transforms + random search best config\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Scenario 1/1 - Epochs: 700 - lr: - 0.0001 - dropout: 0 - Weight_decay: 0.00016051911333587627 - Grad clip: 0.02576638574613588\n",
            "  Run# [1/5] - Num Samples For Training 100 - Num Samples For Val 2000\n",
            "   Train Epoch: 1 [step 1/1 (0%)]\tLoss: 0.993465\n",
            "   Train Epoch: 101 [step 1/1 (0%)]\tLoss: 0.382546\n",
            "   Train Epoch: 201 [step 1/1 (0%)]\tLoss: 0.200414\n",
            "   Train Epoch: 301 [step 1/1 (0%)]\tLoss: 0.074535\n",
            "   Train Epoch: 401 [step 1/1 (0%)]\tLoss: 0.034683\n",
            "   Train Epoch: 501 [step 1/1 (0%)]\tLoss: 0.015287\n",
            "   Train Epoch: 601 [step 1/1 (0%)]\tLoss: 0.016038\n",
            "   Train Epoch: 700 [step 1/1 (0%)]\tLoss: 0.015774\n",
            "   Test set: Average loss: 0.0044, Accuracy: 729/2000 (36.45%)\n",
            "  Run execution time: train: 155.332 (s) - eval: 0.764 (s)- total: 156.096 (s)\n",
            "  Run# [2/5] - Num Samples For Training 100 - Num Samples For Val 2000\n",
            "   Train Epoch: 1 [step 1/1 (0%)]\tLoss: 1.054169\n",
            "   Train Epoch: 101 [step 1/1 (0%)]\tLoss: 0.376461\n",
            "   Train Epoch: 201 [step 1/1 (0%)]\tLoss: 0.170049\n",
            "   Train Epoch: 301 [step 1/1 (0%)]\tLoss: 0.074577\n",
            "   Train Epoch: 401 [step 1/1 (0%)]\tLoss: 0.035534\n",
            "   Train Epoch: 501 [step 1/1 (0%)]\tLoss: 0.021809\n",
            "   Train Epoch: 601 [step 1/1 (0%)]\tLoss: 0.016703\n",
            "   Train Epoch: 700 [step 1/1 (0%)]\tLoss: 0.009904\n",
            "   Test set: Average loss: 0.0042, Accuracy: 759/2000 (37.95%)\n",
            "  Run execution time: train: 154.445 (s) - eval: 0.759 (s)- total: 155.204 (s)\n",
            "  Run# [3/5] - Num Samples For Training 100 - Num Samples For Val 2000\n",
            "   Train Epoch: 1 [step 1/1 (0%)]\tLoss: 0.936893\n",
            "   Train Epoch: 101 [step 1/1 (0%)]\tLoss: 0.373526\n",
            "   Train Epoch: 201 [step 1/1 (0%)]\tLoss: 0.174713\n",
            "   Train Epoch: 301 [step 1/1 (0%)]\tLoss: 0.063958\n",
            "   Train Epoch: 401 [step 1/1 (0%)]\tLoss: 0.034751\n",
            "   Train Epoch: 501 [step 1/1 (0%)]\tLoss: 0.017030\n",
            "   Train Epoch: 601 [step 1/1 (0%)]\tLoss: 0.012998\n",
            "   Train Epoch: 700 [step 1/1 (0%)]\tLoss: 0.012045\n",
            "   Test set: Average loss: 0.0043, Accuracy: 780/2000 (39.00%)\n",
            "  Run execution time: train: 154.273 (s) - eval: 0.771 (s)- total: 155.044 (s)\n",
            "  Run# [4/5] - Num Samples For Training 100 - Num Samples For Val 2000\n",
            "   Train Epoch: 1 [step 1/1 (0%)]\tLoss: 1.045752\n",
            "   Train Epoch: 101 [step 1/1 (0%)]\tLoss: 0.339778\n",
            "   Train Epoch: 201 [step 1/1 (0%)]\tLoss: 0.170568\n",
            "   Train Epoch: 301 [step 1/1 (0%)]\tLoss: 0.077435\n",
            "   Train Epoch: 401 [step 1/1 (0%)]\tLoss: 0.031442\n",
            "   Train Epoch: 501 [step 1/1 (0%)]\tLoss: 0.019418\n",
            "   Train Epoch: 601 [step 1/1 (0%)]\tLoss: 0.010596\n",
            "   Train Epoch: 700 [step 1/1 (0%)]\tLoss: 0.006670\n",
            "   Test set: Average loss: 0.0039, Accuracy: 886/2000 (44.30%)\n",
            "  Run execution time: train: 153.679 (s) - eval: 0.752 (s)- total: 154.431 (s)\n",
            "  Run# [5/5] - Num Samples For Training 100 - Num Samples For Val 2000\n",
            "   Train Epoch: 1 [step 1/1 (0%)]\tLoss: 0.909625\n",
            "   Train Epoch: 101 [step 1/1 (0%)]\tLoss: 0.336025\n",
            "   Train Epoch: 201 [step 1/1 (0%)]\tLoss: 0.190095\n",
            "   Train Epoch: 301 [step 1/1 (0%)]\tLoss: 0.072564\n",
            "   Train Epoch: 401 [step 1/1 (0%)]\tLoss: 0.036186\n",
            "   Train Epoch: 501 [step 1/1 (0%)]\tLoss: 0.022619\n",
            "   Train Epoch: 601 [step 1/1 (0%)]\tLoss: 0.010899\n",
            "   Train Epoch: 700 [step 1/1 (0%)]\tLoss: 0.018374\n",
            "   Test set: Average loss: 0.0045, Accuracy: 734/2000 (36.70%)\n",
            "  Run execution time: train: 153.257 (s) - eval: 0.752 (s)- total: 154.009 (s)\n",
            "  Scenario 1/1 - Epochs: 700 - lr: - 0.0001 - dropout: 0 - Weight_decay: 0.00016051911333587627 - Grad_clip: 0.02576638574613588\n",
            "  Avg execution time: train: 154.197 +- 0.708 (s) - eval: 0.760 +- 0.007 (s) - total: 154.957 +- 0.713 (s) on Tesla P100-PCIE-16GB\n",
            "  Final acc over 5 instances: 38.88 +- 2.86%\n",
            "\n",
            "CPU times: user 11min 24s, sys: 1min 32s, total: 12min 56s\n",
            "Wall time: 12min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}